# graph_visualizer.py

import mlflow
import pandas as pd
import matplotlib.pyplot as plt
import tempfile
import os

# Importa o nome do experimento
try:
    from ..config import EXPERIMENT_NAME
except ImportError:
    print("Erro: N√£o foi poss√≠vel importar EXPERIMENT_NAME de config.py.")
    EXPERIMENT_NAME = "Detec√ß√£o Fraudes - Logistic Regression Baseline"  # Fallback


def create_and_log_metrics_graph():
    """
    Busca todas as runs do MLflow, plota o gr√°fico Precision vs. Recall
    e o salva como um artefato.
    """

    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)
    if not experiment:
        print(f"Erro: Experimento '{EXPERIMENT_NAME}' n√£o encontrado.")
        return

    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])

    # üö® FILTRO PARA GARANTIR QUE APENAS AS RUNS COM AMBAS AS M√âTRICAS SEJAM USADAS
    # Filtramos as runs que n√£o t√™m Precision/Recall (ex: runs de teste que falharam antes)
    runs = runs.dropna(subset=['metrics.precision', 'metrics.recall'])

    print(f"--- Gerando gr√°fico para {len(runs)} runs no experimento: {EXPERIMENT_NAME} ---")

    # --- PLOTAGEM DO GR√ÅFICO PRECISION VS. RECALL ---

    plt.figure(figsize=(10, 8))

    # Eixo Z: Usaremos o F1-Score para colorir os pontos (melhor F1 = cor mais quente)
    # Usamos o AUC-ROC (se existir) como tamanho do ponto
    f1_scores = runs["metrics.f1_score"]

    # O tamanho do ponto ser√° mapeado ao AUC-ROC (quanto maior o AUC, maior o ponto)
    # Se auc_roc n√£o for uniforme (o que ocorre), √© melhor usar F1-Score
    sizes = (f1_scores * 200) + 50  # Escala para visualiza√ß√£o

    scatter = plt.scatter(
        runs["metrics.precision"],
        runs["metrics.recall"],
        c=f1_scores,
        s=sizes,
        cmap='viridis',  # Mapa de cores
        alpha=0.7
    )

    plt.xlabel("Precision")
    plt.ylabel("Recall")
    plt.title("Trade-off Precision vs. Recall (Tamanho = F1-Score)")
    plt.grid(True)

    # Adicionar anota√ß√µes dos nomes das runs
    for index, row in runs.iterrows():
        plt.text(
            row["metrics.precision"],
            row["metrics.recall"],
            f' {row["tags.mlflow.runName"]}',
            fontsize=9,
            ha='left',
            va='center'
        )

    # Adicionar barra de cores para o F1-Score
    cbar = plt.colorbar(scatter)
    cbar.set_label('F1-Score')

    # Ajustar limites do eixo para clareza (0 a 1)
    plt.xlim(0, 1)
    plt.ylim(0, 1)

    # --- SALVAR E LOGAR COMO ARTEFATO ---

    with tempfile.TemporaryDirectory() as tmpdir:
        path = os.path.join(tmpdir, "precision_vs_recall_tradeoff.png")
        plt.savefig(path)

        # Logamos o gr√°fico dentro de uma run (p.ex., a Run 2, que √© a melhor)
        # Para logar artefatos, √© necess√°rio uma run ativa.
        # Salva o gr√°fico na pasta 'artifacts/' para o usu√°rio ver.

        # Para fins de demonstra√ß√£o local, salvamos na raiz:
        final_path = os.path.join(os.getcwd(), "precision_vs_recall_tradeoff.png")
        plt.savefig(final_path)
        print(f"Gr√°fico salvo localmente em: {final_path}")

    plt.show()  # Opcional: mostrar na tela


if __name__ == "__main__":
    # Esta fun√ß√£o n√£o loga o artefato DENTRO de uma run existente,
    # mas a executa de forma aut√¥noma para an√°lise.
    create_and_log_metrics_graph()